# Quasi-experimental, reproducible intervention to improve observer agreement in the classification of distal radius fractures and development of Computerized Adaptive Test

<!-- https://mail.google.com/mail/u/0/?tab=wm#inbox/1480da4c64832eb1 -->


Fabiano Caumo, MD  
Pedro Gaspar Soares Justo, MD   
Henrique Ayzemberg, MD  
Bruno Melo
Joao Ricardo Vissoci
Ana Paula Bonilauri Ferreira, DDS, PhD  
Ricardo Pietrobon, MD, PhD  


## Abstract

<!-- write at the end -->

## Introduction

Although the AO classification is considered one of the main pillars behind the education of orthopedic residents, observer agreement has been reported to be poor in a number of previous publications (@andersen1996classification; @belloti2008distal; @kural2010evaluation; @kucuk2013reliability). Although well established, these findings are disconcerting as classifications are supposed to guide treatment indications, and a substantion lack of observer agreement might mean that a large percentage treatment variability might be explained by a simple lack of understanding of the fracture radiological appearance, biomechanics and, as a consequence, treatment. Despite our knowledge of the cognition behind medical diagnosis having substantially improved over the past couple decades, however, to our knowledge these fields have remained largely without an intersection.

The accurate usage of a scale can be evaluated determining how reproducible are the answers from different observers at different times (@garbuz2002classification). According to this criteria, although AO scale is  widely accepted, several studies have found low agreement rates. (@belloti2008distal; @kural2010evaluation; @kucuk2013reliability). These low agreement rates may be reflex of the use of wrong cognitive schema or wrong heuristics. 

Previous studies have found that physicians tend to use heuristics instead of evidence based instruments such as scales (@ferreira2010clinical) in their daily practice. Since medical education is based on several schemas where knowledge is built for future use (@regehr1996issues), learning a coherent cognitive schema that avoids cognitive overload may facilitate the understanding of scales (@ruiter2012achieve; @ferreira2010clinical), thus enhancing its agreement rates and consequently its accurate daily usage. <!-- Ricardo, please check it out -->

The objective of this study was therefore to investigate the intra and interobserver reliability of the AO classification system for distal radius fractures with and without the aid of a cognitive schemata obtained through a cognitive task analysis.


## Methods


### Institutional Review Board

We obtained approval from the Research Ethics Committee of the São José Municipal Hospital prior to the initiation of this project (protocol number 1240.567). All participants were provided with informed consent prior to enrollment in the study.


### Participants

A total of 14 orthopedic residents participated in this study, 6 were first-year, 4 were second-year, and 4 were third-year residents. Their average age was 29, 13 being males.


### Image selection

Two second-year and one fourth-year residents that were not enrolled directly with the study and an orthopedic surgeon, specialist in hand selected 20 images containing fractures with a wide pattern variation chosen to cover the full spectrum of the AO classification for distal radius fractures. As imagens eram na incidência ântero-posterior e perfil. As imagens foram obtidas de arquivos dos Hospital São Municipal São José (Joinville – SC). Qualquer sinal de identicação de pacientes foi removido. Imagens radiográficas mal posicionadas que poderiam gerar problemas na interpretação foram excluídas. Imagens de baixa qualidade ou com artefatos ou outros defeitos técnicos também foram excluídas.


### AO classification


The AO classification is based on the degree of bone fracture, serving as the basis for both treatment indication and outcome assessment (@belloti2008distal). It divides fractures into three main categories: extra-articular, partial articular and complete articular. These three groups are organized in progressive order of severity in relation to their anatomical severity, treatment complexity and prognosis. Fractures from Group A (extra-articular) do not affect the radiocarpal joint surface at all, while Group B (partial articular fracture) does affect the radiocarpal joint leaving a section of the articular surface remaining connected to the diaphysis. Fractures in Group C (complete articular fracture) constitutes a complete separation between the involved articular surface and the diaphysis. 

These three main groups are then subdivided into three subgroup, therefore constituting a total of 27 different types of fractures. These fractures vary in relation to how stable they are, their degree of comminution, how reduceable they are and the localization of their fragments (Müeller, 1987). https://www2.aofoundation.org/wps/portal/surgery?showPage=diagnosis&bone=Radius&segment=Distal )



### Situated schemata extraction using Cognitive Task Analysis

For the purposes of our paper, we define a situated schema as the collection of concepts and situations (e.g., narratives) that an expert hand surgeon relates to each classification category. In order to extract the situated schema from our expert hand surgeon (HA), we used the following sequence. 

First, the AO classification was presented to the hand surgery expert in an electronic format combining text and graphics for each classification category. Second, we asked the surgeon to "think aloud" about what they thought when finding a case in their daily practice. After an initial description, we specifically asked the expert to discuss any diagnostic, biomechanical or related therapeutic decision if it had not yet been previously mentioned. We also encouraged the expert to provide any narratives that might occur to him while thinking aloud about each classification category. The entire process was recorded in a video.

Second, the video was analyzed and a graph constituted by nodes and edges was built using [Graphviz](http://www.graphviz.org/). Each node represented either a concept or a situation, while edges connected relationships among diagnostic, biomechanical and therapeutic nodes.



### Study logistics and procedures

#### Baseline evaluation
At baseline, all participants independently classified all 20 images according to the AO classification. All participants simultaneously gathered in a single room, being instructed not to look at each other's responses or discuss any cases. Each resident received a directory with all images to be classified. All responses were provided in paper sheets, which were subsequently transcribed to a database. Residents were allowed to check the Web for the classification.There was no time limit for making decisions. Study authors did not participate as study subjects. The directory with all images was deleted at the end of the sesssion in order to decrease the odds of recall bias in subsequent evaluations.



#### Pre-intervention, thirty-day evaluation

After 30 days, each study participant received a new directory with the same 20 images, but in a different order. All other procedures were executed exactly as described for the baseline session.

<!-- 
not sure we are going to describe this as it will get confusing

After the classification was finalized, Após finalizada a classificação das 20 imagens, já em seguida eles realizaram 30 exercícios no Concerto. E imediatamente após a realização dos exercícios as 20 imagens foram reclassificadas, dando aos residentes a oportunidade de mudar sua classificação. Assim que finalizaram a reclassificação das imagens o arquivo foi deletado de seus computadores.  -->

<!-- Concordo. -->



#### Intervention
The educational intervention was constituted by weekly sessions where participants completed 15 exercises related to the diagnosis, biomechanics and therapeutic planning of radius fractures. Os residentes, após terem respondido cada exercício, eram apresentados à resposta correta, juntamente com uma explicação justificando tal resposta. Os exercícios estavam organizados, na Plataforma online Edx, em “blocos” de 15 perguntas cada, ou seja, foram criados 4 blocos correspondentes as quatro semanas de intervenção. Os blocos de perguntas foram programados para serem “realised” semanalmente, isto é, os residentes tinham acesso somente a um bloco de perguntas por semana. The full spectrum described by the AO classification was covered based on cognitive schema from our expert hand surgeon (HA) as described in the previous session - Situated schemata extraction using Cognitive Task Analysis.

<--!—Ricardo, please check it out. -->



#### Post-intervention, sixty-day evaluation

After the 4-week intervention period, all participants classified the same 20 images following the same protocol.





### Outcome measurement

Intra-observer agreement was measured by comparing ratings by the same participant between baseline and the thirty 30-day assessment. Baseline inter-observer agreement was measured at the 30-day assessment. The pre-post intervention evaluation was conducted by comparing the 30-day pre-intervention assessment with the 60-day post-intervention assessment.


### Data analysis

All data were extracted directly from [MySQL](http://www.mysql.com/) and [MongoDB](http://www.mongodb.org/) databases connected to the [Open edX](http://code.edx.org/) platforms. Data sets were then merged, also undergoing an exploratory graphical analysis to verify distributions, percentages, means and frequencies/percentages as well as rates of missing data. 

Apenas observadores que haviam completado um determinado grupo de observações (dia 1, dia 30 ou dia 60) foram considerados na análise. Porcentagens de concordância assim como valores de Kappa Fleiss foram reportados. Kappa Fleiss é uma medida de concordância para variáveis categóricas que leva em consideração a possível concordância ao acaso. @fleiss1973equivalence  Por fim, a comparação entre os valores de Kappa pré e pós intervenção (dias 30 e 60, respectivamente) foram estimadas através da computação de erros padrão e intervalos de confiança (95%) utilizando bootstrap. <!-- check with joao for reference -->


<!-- @article{fleiss1973equivalence,
  title={The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability.},
  author={Fleiss, Joseph L and Cohen, Jacob},
  journal={Educational and psychological measurement},
  year={1973},
  publisher={Sage Publications}
} -->



<!-- ### Computerized Adaptive Test -->

<!-- this section won't go into the SBOT -->

<!-- Based on the values obtained for the interobserver agreement for each individual fracture, we developed a free, publicly available online Computerized Adaptive Test. In this system, learners will initially be asked to rate an image with average observer agreement. Depending on their response agreeing or not with the majority of responders, they are taken to, respectively, a subsequent image with a lower or greater degree of interobserver ("harder" or "easier" images).  -->

## Results

<!-- http://goo.gl/b15W3W -->



### Resultados descritivos

Quando todos os 11 observadores completando as 3 avaliações foram considerados, não houve nenhuma instância em que houvesse concordância completa entre todos, mesmo quando houve uma maior tolerancia em relação a subclassificações. No entanto, a porcentagem de concordância entre os 14 observadores completando a primeira avaliação com uma tolerância em relação à subclassificação, houve uma concordância em 70% das avaliações. Esta concordância caiu para 50% e 45% nas avaliações dos dias 30 (14 observadores) e 60 (15 observadores, pós-intervenção), respectivamente. 

Em relação à comparação das observações do mesmo observador, comparando os dias 1 e 30, a porcentagem de concordância foi de 11.2% sem tolerância e 78.6% com tolerância. 


### Valores de kappa

Valores de Kappa Fleiss para a concordância entre observadores amostra do dia 1 foram de 0.225 (p < 0.001), 0.212 (p < 0.001) para o dia 30 e 0.214 (p < 0.001) para o dia 60. Não houve uma diferença estatisticamente significativa entre as concordâncias do dia 30 (pré-intervenção) e dia 60 (pós-intervenção). A concordância entre o mesmo observador nos dias 1 e 30 demonstrou um valor de Kappa Fleiss de -0.004.



## Discussão


Até onde sabemos, esse é o primeiro estudo avaliando uma intervenção na tentativa de melhorar o grau de concordância entre observadores para a classificação da AO para o terço distal do rádio. Nossos resultados mostraram uma concordância estável durante o estudo, não tendo sido alterada em decorrência da intervenção. Também encontramos que o grau de concordância é melhorado quando a classificação é simplificada através da retirada das subclassificações. 

O baixo grau de concordância entre observadores nesse estudo está alinhado com a literatura sobre classificações de fraturas ortopédicas (@andersen1996classification; @belloti2008distal; @kural2010evaluation; @kucuk2013reliability). Essa baixa concordância se dá em grande parte pela complexidade das classificações, o que dá margem a interpretações diversas, especialmente por profissionais em treinamento e portanto com menos experiência (@kreder1996consistency; @arealis2014does). Apesar de que a intenção de se criar uma classificação que seja clinicamente detalhada é inicialmente interessante, a alta carga cognitiva exigida dos profissionais que a irão utilizar tende a fazer com que ela perca a sua praticidade. Esforços deveriam ser realizados portanto para a criação de escalas que sejam mais dinamicamente adaptadas a profissionais com diferentes graus de experiência na interpretação radiográfica. Por exemplo, profissionais que trabalhem em pronto socorros deveriam utilizar uma escala mais simplificada, enquanto sub-especialistas deveriam utilizar escalas mais detalhadas. O grau de detalhamento em cada uma destas subescalas seria definido através de estudos que identifiquem o grau de concordância obtido na prática clínica diária.


Apesar da nossa intervenção ter sido baseada em mecanismos bem estabelecidos na literatura sobre esquemas cognitivos (@regehr1996issues; @ruiter2012achieve), não houve uma melhora da concordância como nós havíamos hipotetizado. Esquemas cognitivos situados hipotetizam que o cérebro raciocina não apenas através de informações armazenadas no próprio cérebro, mas utilizando fatores ambientais como tecnologias, contatos sociais, entre outros fatores (@van2010cognitive). Causas para a não melhora provavelmente se devem ao baixo tempo de exposição em relação à intervenção <!-- ref -->, e também ao fato de que esta exposição não ocorreu em um contexto clínico, mas sim em um ambiente educacional artificial. Resultados superiores talvez pudessem ter sido encontrados se a intervenção educacional pudesse ter ocorrido durante a prática clínica diária, especificamente no momento em que os participante estivessem atendendo pacientes com fraturas de radio distal. 

Apesar de o nosso artigo ser, até onde sabemos, o primeiro a conduzir uma intervenção na tentativa de melhorar o grau de concordância entre observadores, o nosso estudo tem limitações. Primeiro, a nossa intervenção utilizou um desenho pré-pós ao invés de um estudo randomizado. Na ausência de randomização, nós, portanto, não podemos fazer afirmações sobre relações causais entre a intervenção e a ausência de impacto sobre o grau de concordância. Estudos randomizados requerem no entanto amostras significativamente maiores, o que pode levar a barreiras logísticas em relação à sua execução. Segundo, esse estudo foi restrito a um grupo de participantes de uma única instituição, o que limita a sua generalizabilidade. Enquanto a participação de múltiplas instituições é sempre desejável, sociedades profissionais em ortopedia ainda não estão logisticamente organizadas para permitir estudos de maior escala, o que dificulta a sua realização. Por último, intervenções mais prolongadas e contextualizadas na prática diária teriam sido desejáveis, mas como nas limitações anteriores, a sua execução é limitada por fatores logísticos. 

Em conclusão, nós não recomendamos que intervenções educacionais curtas e descontextualizadas da prática clínica sejam utilizadas no aprendizado de classificações complexas. No entanto, a simplificação de tais classificações deveria ser considerada, levando a uma personalização da escala a ser utilizada a grupos de profissionais com graus de experiência diferentes. No que diz respeito a estudos futuros, recomendamos a utilização de estudos randomizados que permitam investigações causais, assim como a contextualização e personalização das intervenções educacionais. 






@illarramendi1998evaluation

Interobserver agreement: average kappa coefficient was 0.37  which corresponds to a fair reproducibility.
Experienced observers achieved an average kappa coefficient of 0.31 (fair reproducibility). The non-specialists obtained an average kappa coefficient of 0.40 (fair reproducibility).
Intra: the average kappa coefficient was 0.57, representing moderate reproducibility).
For experienced observers, the kappa coefficient was 0.50 (moderate reproducibility). The non-specialists obtained the kappa coefficient of 0.63 (good reproducibility). 
This can be attributed to several factors: firstly, the classification is complex and differences between subgroups can sometimes be subtle; secondly, there are no strict guidelines for assessment of the extent of metaphyseal and articular comminution, and thirdly, the determination of the limits of the distal radioulnar joint.

@andersen1996classification

Inter e Intra: fair agreement
Inter: Reducing it to nine groups or three general types (Table 3) had the effect of increasing the interobserver agreement (0.297 and 0.636, respectively).
Intra: Reducing the number of groups to nine (Table 6) resulted in modest increases in the kappa values (range, 0.341-0.449). Further reduction to the three main types yielded kappa values of 0,583-0.701.

@belloti2008distal

We used groups and subgroups (nine types) and the mean intraobserver value was unsatisfactory (0.49). There was a significant difference between the values for the more experienced observers (OHS  = 0.64 and OT  = 0.64) and those for the less experienced ones (R3  = 0.4835 and UG6  = 0.3751). This suggests that the expertise level had an influence. 
The interobserver agreement was also unsatisfactory, but presented progressive increase from T1 (0.27) to T3 (0.31). 
Variation in evaluators’ expertise may have influenced evaluations carried out on intraobserver and interobserver agreement. Studies have shown that less experienced observers attain lower rates of intraobserver agreement than do expert physicians.12,16 However, in a comparison of one group in which the observers were more experienced in rating assessments with another group whose expertise was lower, no significant difference in interobserver agreement was found.12

@kreder1996consistency

Agreement was best for AO type and decreased progressively for AO group and subgroup. For AO type, interobserver agreement improved with increasing experience.
Attending surgeons achieved the highest interobserver agreement scores for the AO classification categories with an SAV value of 0.68 (substantial agreement) for the AO type; 0.48 (moderate agreement) for the nine possible AO groups; and 0.33 (fair agreement) for the 27 potential AO subgroups.

@kucuk2013reliability

Reliability kappa values for for all classification systems in both resident and surgeon groups after
1st and 2nd evaluations. AO 0.29 (1st evaluation – resident) 0.31 (2nd evaluation – resident) 0.30 (1st evaluation – surgeon) 0.34 (2nd evaluation – surgeon)

@kural2010evaluation

Intraobserver Kappa Values after First and Second Evaluations of Observers= kappa 0.309
Interobserver Kappa Values of First And Second Readings for Each Classification= kappa 0.0961

@van2010agreement

Interobserver agreement for Type level was moderate (ktype=0.60), then decreased when the classification was expanded to Group and Subgroup grading. Interobserver agreement for Group level was moderate (kgroup=0.41) and Subgroup was fair (ksubgroup=0.33).

@ploegmakers2007four

The weighted kappa values for intraobserver reproducibility were 0.52 for the AO/ASIF.

@arealis2014does

Intraobserver agreement between X-ray and CT scan for the classification of distal radius fractures kappa=0.65
Inter-observer agreement when X-ray is used for the classification of distal radius fractures. – k= 0.299
Inter-observer agreement when CT scan is used for the classification of distal radius fractures. – k= 0.299

@flinkkila1998poor

The overall kappa coefficients were 0.18 for plain radiographs and 0.16 for CT, indicating
poor interobserver reliability in both cases. After reducing the categories to five main classes, the overall kappa coefficients were 0.23 using plain radiographs and 0.25 with the addition  of CT  showing a slight increase in reliability after adding CT, but the result was still regarded as only poor to fair. After reducing the classes to two main types, the percentage agreement varied from 60% to 87% for plain radiographs and from 87% to 100% for CT. The overall kappa coefficients were 0.48 (moderate) for plain radiographs and 0.78 (good to excellent) with CT.







To the best of our knowledge, this is the first study creating a computerized adaptive test for the AO radius fracture classification. We found that 

<!-- inter-observer reliability
intra-observer reliability
degree of improvement in inter-observer reliability after training
CAT
 -->

Despite being the first to report a computerized adaptive test for the AO radius fracture classification, our study has limitations. First, our analysis was restricted to a group of residents, and different groups might present a variation in observer agreement serving as the basis for the Computerized Adaptive Test. Second, while selecting images for the observer agreement study, we chose to select a wide spectrum of fractures rather than representing the actual prevalence of these fractures in an average practice. 




http://goo.gl/NwMNN
http://goo.gl/17QP



## References




---


# mysql commands

## below to both create root pwd as well as login as root
mysql --user=root --password=
CREATE USER 'rpietro'@'localhost' IDENTIFIED BY '';

CREATE DATABASE radius;
SHOW DATABASES;
-- DROP DATABASE radius;

mysql -u root -p  radius < staging.sql


SELECT VERSION();
HELP;
HELP contents;
quit


USE radius;
SELECT DATABASE();
SHOW TABLES;
SELECT * FROM radius.courseware_studentmodule WHERE module_type = 'problem'  INTO OUTFILE '/Users/rpietro/Desktop/ana2.csv';



# mongodb commands

mongorestore edxapp

show databases
use edxapp
db
show collections

/*
fs.chunks
fs.files
modulestore
modulestore.location_map
system.indexes
*/

db..findOne()


---


# update.packages()

library(irr)
library(Agreement)
library(vcd)
require(lpSolve)
require(kappaSize)
require(boot)

# sample size

# http://www.ncbi.nlm.nih.gov/pubmed/22560852 - http://cran.r-project.org/web/packages/kappaSize/kappaSize.pdf
# https://etd.library.emory.edu/view/record/pid/emory:7t409



# descriptive

setwd("/Users/rpietro/articles/radius_agreement")

# all analyses only conducted with observers who had completed all phases of the analysis
radius_inter_all  <- read.csv("radius_inter_all.csv")
# radius_inter_all
agree(radius_inter_all)     # Simple percentage agreement
agree(radius_inter_all, 12)  # Extended percentage agreement


radius_inter_day1  <- read.csv("radius_inter_day1.csv")
# radius_inter_day1
agree(radius_inter_day1)     # Simple percentage agreement
agree(radius_inter_day1, 12)  # Extended percentage agreement

radius_inter_day30  <- read.csv("radius_inter_day30.csv")
# radius_inter_day30
agree(radius_inter_day30)     # Simple percentage agreement
agree(radius_inter_day30, 12)  # Extended percentage agreement

radius_inter_day60  <- read.csv("radius_inter_day60.csv")
# radius_inter_day60
agree(radius_inter_day60)     # Simple percentage agreement
agree(radius_inter_day60, 12)  # Extended percentage agreement

# Intra-observer

radius_intra130  <- read.csv("radius_intra130.csv")
# radius_intra130
agree(radius_intra130)     # Simple percentage agreement
agree(radius_intra130, 12)  # Extended percentage agreement

radius_intra3060  <- read.csv("radius_intra3060.csv")
# radius_intra3060
agree(radius_intra3060)     # Simple percentage agreement
agree(radius_intra3060, 12)  # Extended percentage agreement
# SexualFun
# (K <- Kappa(SexualFun))
# confint(K)
# agree <- agreementplot(SexualFun, main="Is sex fun?")
# We have thus produced an agreement plot, also called a Bangdiwala's Observer Agreement Chart. Note that our agreement plot is a representation of a k x k confusion matrix. The observed and expected diagonal elements are represented by superposed black and white rectangles, respectively. The extent to which the rectangles are above or below the line indicates the extent of any disagreement. (above and/or below indicates direction of the disagreement). The function also computes two statistic measuring the strength of agreement (relation of respective area sums). The first statistic is accessed using the term Bandiwala. This statistic is the unweighted agreement strength statistic. The second statistic makes an adjustment for ordered ratings, and is accessed using the code Bangdiwala_Weighted. Both statistics are measured on a scale from 0 to 1, where 1 indicates perfect agreement and 0 indicates perfect disagreement.
# unlist(agree)


# Interobserver

# data(diagnoses)
# head(diagnoses)
# df <- diagnoses[,1:3]
# head(df)
kappam.fleiss(radius_inter_day1)
kappam.fleiss(radius_inter_day30)
kappam.fleiss(radius_inter_day60)
kappam.fleiss(radius_intra130)
kappam.fleiss(radius_intra3060)


# The unified approach calculates the agreement statistics for both continuous and categorical data to cover multiple readings from each of the n subjects.
# data(DCLHb);
# head(DCLHb)
# ua <- unified.agreement(dataset=DCLHb, var=c("HEMOCUE1","HEMOCUE2","SIGMA1","SIGMA2"), k=2, m=2, CCC_a_intra=0.9943, CCC_a_inter=0.9775, CCC_a_total=0.9775, CP_a=0.9, tran=1, TDI_a_intra=75, TDI_a_inter=150, TDI_a_total=150, error="const", dec=1, alpha=0.05);
# summary(ua);


#to obtain a 95%confidence interval of the four classification systems, using the boot package
ckappa.boot <- function(data,x) {ckappa(data[x,])[[2]]}
icsp <- boot(olecranon_speccoltvsnonspeccolt,ckappa.boot,1000)
quantile(icsp$t,c(0.025,0.975)) # two-sided bootstrapped confidence interval of kappa
boot.ci(icsp,type="bca") # adjusted bootstrap percentile (BCa) confidence interval (better)
icnsp <- boot(olecranon_specschatvsnonspecschat,ckappa.boot,1000)
quantile(icnsp$t,c(0.025,0.975)) # two-sided bootstrapped confidence interval of kappa
boot.ci(icnsp,type="bca") # adjusted bootstrap percentile (BCa) confidence interval (better)
icsp <- boot(olecranon_specmayvsnonspecmay,ckappa.boot,1000)
quantile(icsp$t,c(0.025,0.975)) # two-sided bootstrapped confidence interval of kappa
boot.ci(icsp,type="bca") # adjusted bootstrap percentile (BCa) confidence interval (better)
icnsp <- boot(olecranon_specaovsnonspecao,ckappa.boot,1000)
quantile(icnsp$t,c(0.025,0.975)) # two-sided bootstrapped confidence interval of kappa
boot.ci(icnsp,type="bca") # adjusted bootstrap percentile (BCa) confidence interval (better)

